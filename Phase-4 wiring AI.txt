Phase-4: wiring AI üîßüß†
Goal: server-side only keys, Claude primary (Sonnet 3.5 today; easily switch when Anthropic ships ‚ÄúSonnet-4‚Äù), OpenAI fallback, and a model picker UI. We‚Äôll also stub connectors for Gemini/Grok so adding them later is one-file work (no rewire).

Reality check: ‚ÄúClaude Sonnet 4‚Äù and ‚ÄúGPT-5‚Äù aren‚Äôt public models as of today. I‚Äôm giving you a provider registry that will accept those names, and either route to the closest current model or use the Mock provider if no key/SDK exists. When those models are real, you‚Äôll just update the map.

0) Install deps

From repo root:

pnpm add -F @kiroclone/server @anthropic-ai/sdk openai langchain zod sharp
pnpm add -D -F @kiroclone/server @types/sharp
pnpm add -F @kiroclone/app-renderer @reduxjs/toolkit react-redux

1) Update env template
.env.example (append)
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
# Optional future providers (stubs for now):
GOOGLE_API_KEY=        # Gemini
XAI_API_KEY=           # Grok


(Put your real keys in .env, which stays local.)

2) Server: provider registry + /chat + /spec (LangChain)
packages/server/src/providers/types.ts (new)
export type ChatMessage = { role: 'system' | 'user' | 'assistant'; content: string };
export type ChatRequest = {
  provider: 'anthropic' | 'openai' | 'gemini' | 'grok' | 'auto' | 'mock';
  model: string;         // e.g., 'claude-3-5-sonnet-20241022', 'gpt-4o', 'gpt-5'
  messages: ChatMessage[];
  maxTokens?: number;
};

export type ChatResponse = {
  provider: string;
  model: string;
  content: string;
  usage?: Record<string, unknown>;
  warn?: string;
};

export interface ChatProvider {
  id: string;
  supports(model: string): boolean;
  available(): boolean;
  chat(req: ChatRequest): Promise<ChatResponse>;
}

packages/server/src/providers/anthropic.ts (new)
import Anthropic from '@anthropic-ai/sdk';
import { ChatProvider, ChatRequest, ChatResponse } from './types';

const MAP: Record<string, string> = {
  // Friendly aliases -> Anthropic model ids
  'claude-sonnet-4': 'claude-3-5-sonnet-20241022',  // future-safe alias
  'claude-3.5-sonnet': 'claude-3-5-sonnet-20241022',
  'claude-3.5-sonnet-latest': 'claude-3-5-sonnet-20241022',
};

export class AnthropicProvider implements ChatProvider {
  id = 'anthropic';
  private client?: Anthropic;

  constructor() {
    const key = process.env.ANTHROPIC_API_KEY;
    if (key) this.client = new Anthropic({ apiKey: key });
  }

  available() { return !!this.client; }

  supports(model: string) {
    const m = MAP[model] ?? model;
    return /^claude/i.test(m);
  }

  async chat(req: ChatRequest): Promise<ChatResponse> {
    if (!this.client) {
      return { provider: this.id, model: req.model, content: '(no ANTHROPIC_API_KEY set)', warn: 'provider_unavailable' };
    }
    const model = MAP[req.model] ?? req.model;
    const sys = req.messages.find(m => m.role === 'system')?.content;
    const userTurns = req.messages.filter(m => m.role !== 'system');

    const msg = await this.client.messages.create({
      model,
      max_tokens: req.maxTokens ?? 2048,
      system: sys,
      messages: userTurns.map(m => ({ role: m.role === 'assistant' ? 'assistant' : 'user', content: m.content }))
    });

    const content = msg.content?.map(c => ('text' in c ? c.text : '')).join('\n') ?? '';
    return { provider: this.id, model, content, usage: { input: msg.usage?.input_tokens, output: msg.usage?.output_tokens } };
  }
}

packages/server/src/providers/openai.ts (new)
import OpenAI from 'openai';
import { ChatProvider, ChatRequest, ChatResponse } from './types';

const MAP: Record<string, string> = {
  'gpt-5': 'gpt-4o', // alias until a GPT-5 is real
  'gpt-4o': 'gpt-4o',
  'gpt-4o-mini': 'gpt-4o-mini'
};

export class OpenAIProvider implements ChatProvider {
  id = 'openai';
  private client?: OpenAI;

  constructor() {
    const key = process.env.OPENAI_API_KEY;
    if (key) this.client = new OpenAI({ apiKey: key });
  }

  available() { return !!this.client; }

  supports(model: string) {
    const m = MAP[model] ?? model;
    return /^gpt/i.test(m);
  }

  async chat(req: ChatRequest): Promise<ChatResponse> {
    if (!this.client) {
      return { provider: this.id, model: req.model, content: '(no OPENAI_API_KEY set)', warn: 'provider_unavailable' };
    }
    const model = MAP[req.model] ?? req.model;
    const resp = await this.client.chat.completions.create({
      model,
      max_tokens: req.maxTokens ?? 2048,
      messages: req.messages.map(m => ({ role: m.role, content: m.content }))
    });
    const choice = resp.choices?.[0]?.message?.content ?? '';
    return { provider: this.id, model, content: choice, usage: resp.usage as any };
  }
}

packages/server/src/providers/stubs.ts (new)
import { ChatProvider, ChatRequest, ChatResponse } from './types';

export class MockProvider implements ChatProvider {
  id = 'mock';
  available() { return true; }
  supports(_m: string) { return true; }
  async chat(req: ChatRequest): Promise<ChatResponse> {
    const last = req.messages.filter(m => m.role !== 'system').at(-1)?.content ?? '';
    return { provider: 'mock', model: req.model, content: `[MOCK REPLY] ${last}` };
  }
}

// Future placeholders
export class GeminiProvider extends MockProvider { id = 'gemini'; }
export class GrokProvider extends MockProvider { id = 'grok'; }

packages/server/src/providers/index.ts (new)
import { ChatProvider, ChatRequest } from './types';
import { AnthropicProvider } from './anthropic';
import { OpenAIProvider } from './openai';
import { GeminiProvider, GrokProvider, MockProvider } from './stubs';

const providers: ChatProvider[] = [
  new AnthropicProvider(),
  new OpenAIProvider(),
  new GeminiProvider(), // stubbed until hooked to SDK
  new GrokProvider(),   // stubbed until hooked to SDK
  new MockProvider()
];

export function resolveProvider(req: ChatRequest): ChatProvider {
  if (req.provider && req.provider !== 'auto') {
    const p = providers.find(p => p.id === req.provider);
    return p ?? new MockProvider();
  }
  // auto: prefer Anthropic -> OpenAI -> others -> mock
  const preferred = ['anthropic','openai','gemini','grok'];
  for (const id of preferred) {
    const p = providers.find(p => p.id === id);
    if (p && p.available() && p.supports(req.model)) return p;
  }
  // fall back by first available
  const any = providers.find(p => p.available());
  return any ?? new MockProvider();
}

packages/server/src/routes/chat.ts (new)
import { Router } from 'express';
import { z } from 'zod';
import { resolveProvider } from '../providers';
import type { ChatRequest } from '../providers/types';

const ChatSchema = z.object({
  provider: z.enum(['anthropic','openai','gemini','grok','auto','mock']).default('auto'),
  model: z.string().default('claude-3.5-sonnet'),
  messages: z.array(z.object({
    role: z.enum(['system','user','assistant']),
    content: z.string()
  })),
  maxTokens: z.number().int().positive().optional()
});

export const chatRouter = Router();

chatRouter.post('/', async (req, res) => {
  const parsed = ChatSchema.safeParse(req.body);
  if (!parsed.success) return res.status(400).json({ error: parsed.error.flatten() });

  const reqObj = parsed.data as ChatRequest;
  const provider = resolveProvider(reqObj);
  try {
    const out = await provider.chat(reqObj);
    res.json(out);
  } catch (e: any) {
    res.status(500).json({ error: e?.message ?? 'chat_failed', provider: provider.id });
  }
});

packages/server/src/routes/spec.ts (new)
import { Router } from 'express';
import { z } from 'zod';
import { resolveProvider } from '../providers';
import type { ChatRequest } from '../providers/types';

/**
 * Minimal ‚Äúspec generator‚Äù chain:
 *  - Turn a user prompt into: requirements, design (mermaid), and a task list.
 *  - This is a single LLM call for now; we‚Äôll expand into a LangChain multi-step later.
 */
export const specRouter = Router();

const Body = z.object({
  prompt: z.string(),
  provider: z.enum(['anthropic','openai','auto','mock']).default('auto'),
  model: z.string().default('claude-3.5-sonnet'),
});

const SYS = `You are a senior architect. 
Given a product idea, produce:
1) REQUIREMENTS (bullet list)
2) SYSTEM DESIGN (with a Mermaid diagram fenced as \`\`\`mermaid ... \`\`\`)
3) TASKS as JSON with: id, title, description, dependsOn[].
Keep it concise and practical for a single repo.`;

specRouter.post('/', async (req, res) => {
  const parsed = Body.safeParse(req.body);
  if (!parsed.success) return res.status(400).json({ error: parsed.error.flatten() });

  const { prompt, provider: pid, model } = parsed.data;
  const cr: ChatRequest = {
    provider: pid,
    model,
    messages: [
      { role: 'system', content: SYS },
      { role: 'user', content: prompt }
    ]
  };
  try {
    const out = await resolveProvider(cr).chat(cr);
    res.json({ ...out, kind: 'spec' });
  } catch (e: any) {
    res.status(500).json({ error: e?.message ?? 'spec_failed' });
  }
});

packages/server/src/app.ts (replace to mount routes)
import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import morgan from 'morgan';
import dotenv from 'dotenv';
import { chatRouter } from './routes/chat';
import { specRouter } from './routes/spec';

export function createApp() {
  dotenv.config();
  const app = express();
  app.use(helmet());
  app.use(cors({ origin: ['http://localhost:5173','http://localhost:5174'] }));
  app.use(express.json({ limit: '15mb' }));
  app.use(morgan('tiny'));

  app.get('/health', (_req, res) => res.json({ ok: true, name: 'KiroClone Server', phase: 4 }));

  app.use('/chat', chatRouter);
  app.use('/generate/spec', specRouter);

  return app;
}


(Keep src/index.ts as in Phase-3.)

3) Renderer: Redux store + Chat sidebar UI
packages/app-renderer/src/store/index.ts (new)
import { configureStore } from '@reduxjs/toolkit';
import settings from './settingsSlice';
import chat from './chatSlice';

export const store = configureStore({ reducer: { settings, chat } });
export type RootState = ReturnType<typeof store.getState>;
export type AppDispatch = typeof store.dispatch;

packages/app-renderer/src/store/settingsSlice.ts (new)
import { createSlice, PayloadAction } from '@reduxjs/toolkit';

type Provider = 'anthropic' | 'openai' | 'auto' | 'mock';

type SettingsState = {
  provider: Provider;
  model: string;
};

const initialState: SettingsState = {
  provider: 'auto',
  // Default to Claude Sonnet; alias resolves on server
  model: 'claude-3.5-sonnet'
};

const slice = createSlice({
  name: 'settings',
  initialState,
  reducers: {
    setProvider(state, action: PayloadAction<Provider>) { state.provider = action.payload; },
    setModel(state, action: PayloadAction<string>) { state.model = action.payload; }
  }
});

export const { setProvider, setModel } = slice.actions;
export default slice.reducer;

packages/app-renderer/src/store/chatSlice.ts (new)
import { createSlice, PayloadAction } from '@reduxjs/toolkit';

export type Msg = { role: 'user' | 'assistant'; content: string };

type ChatState = { messages: Msg[]; busy: boolean; error?: string };
const initial: ChatState = { messages: [], busy: false };

const slice = createSlice({
  name: 'chat',
  initialState: initial,
  reducers: {
    push(state, action: PayloadAction<Msg>) { state.messages.push(action.payload); },
    setBusy(state, action: PayloadAction<boolean>) { state.busy = action.payload; },
    setError(state, action: PayloadAction<string | undefined>) { state.error = action.payload; },
    clear(state) { state.messages = []; state.error = undefined; }
  }
});

export const { push, setBusy, setError, clear } = slice.actions;
export default slice.reducer;

packages/app-renderer/src/main.tsx (wrap App with Provider)
import React from 'react';
import { createRoot } from 'react-dom/client';
import App from './App';
import './styles.css';
import { Provider } from 'react-redux';
import { store } from './store';

declare global { interface Window { kirobridge?: any; } }

createRoot(document.getElementById('root')!).render(
  <Provider store={store}>
    <App />
  </Provider>
);

packages/app-renderer/src/ui/ChatSidebar.tsx (new)
import React, { useState } from 'react';
import { useDispatch, useSelector } from 'react-redux';
import type { RootState } from '../store';
import { push, setBusy, setError } from '../store/chatSlice';
import { setProvider, setModel } from '../store/settingsSlice';

const modelsByProvider: Record<string, string[]> = {
  auto: ['claude-3.5-sonnet','gpt-4o','gpt-5','claude-sonnet-4'],
  anthropic: ['claude-3.5-sonnet','claude-sonnet-4'],
  openai: ['gpt-4o','gpt-4o-mini','gpt-5'],
  mock: ['mock-echo']
};

export const ChatSidebar: React.FC = () => {
  const dispatch = useDispatch();
  const { messages, busy, error } = useSelector((s: RootState) => s.chat);
  const { provider, model } = useSelector((s: RootState) => s.settings);
  const [input, setInput] = useState('');

  const send = async () => {
    if (!input.trim() || busy) return;
    dispatch(push({ role: 'user', content: input }));
    dispatch(setBusy(true));
    dispatch(setError(undefined));
    setInput('');

    try {
      const res = await fetch('http://127.0.0.1:4455/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ provider, model, messages: [{ role:'user', content: input }] })
      });
      const data = await res.json();
      if (!res.ok) throw new Error(data?.error || 'chat_failed');
      dispatch(push({ role: 'assistant', content: data.content || '' }));
    } catch (e: any) {
      dispatch(setError(e?.message ?? 'chat_failed'));
    } finally {
      dispatch(setBusy(false));
    }
  };

  const genSpec = async () => {
    if (!input.trim() || busy) return;
    dispatch(setBusy(true)); dispatch(setError(undefined));
    try {
      const res = await fetch('http://127.0.0.1:4455/generate/spec', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ provider, model, prompt: input })
      });
      const data = await res.json();
      if (!res.ok) throw new Error(data?.error || 'spec_failed');
      dispatch(push({ role: 'assistant', content: data.content || '' }));
    } catch (e: any) {
      dispatch(setError(e?.message ?? 'spec_failed'));
    } finally {
      dispatch(setBusy(false));
    }
  };

  const modelOptions = modelsByProvider[provider] ?? ['claude-3.5-sonnet'];

  return (
    <div style={{ display:'flex', flexDirection:'column', height:'100%' }}>
      <div style={{ fontWeight:700, marginBottom:8 }}>Chat</div>
      <div style={{ display:'grid', gridTemplateColumns:'1fr 1fr', gap:6 }}>
        <select value={provider} onChange={e => dispatch(setProvider(e.target.value as any))}>
          <option value="auto">Auto</option>
          <option value="anthropic">Claude</option>
          <option value="openai">OpenAI</option>
          <option value="mock">Mock</option>
        </select>
        <select value={model} onChange={e => dispatch(setModel(e.target.value))}>
          {modelOptions.map(m => <option key={m} value={m}>{m}</option>)}
        </select>
      </div>
      <div style={{ flex:1, overflow:'auto', marginTop:8, background:'#0b0f14', border:'1px solid #222', padding:8 }}>
        {messages.length === 0 && <div style={{ color:'var(--muted)' }}>Type a request and hit Send, or use ‚ÄúGen Spec‚Äù.</div>}
        {messages.map((m, i) => (
          <div key={i} style={{ margin:'8px 0' }}>
            <div style={{ fontSize:12, color:'#aaa' }}>{m.role}</div>
            <pre style={{ whiteSpace:'pre-wrap', margin:0 }}>{m.content}</pre>
          </div>
        ))}
      </div>
      {error && <div style={{ color:'#f85149', fontSize:12, marginTop:6 }}>{error}</div>}
      <div style={{ display:'flex', gap:6, marginTop:8 }}>
        <input
          value={input}
          onChange={e => setInput(e.target.value)}
          onKeyDown={e => (e.key === 'Enter' && !e.shiftKey ? (e.preventDefault(), send()) : null)}
          placeholder="Ask anything‚Ä¶"
          style={{ flex:1, padding:'8px 10px', background:'#0b0f14', border:'1px solid #222', color:'var(--text)' }}
        />
        <button onClick={send} disabled={busy}>Send</button>
        <button onClick={genSpec} disabled={busy}>Gen Spec</button>
      </div>
    </div>
  );
};

Update layout usage in packages/app-renderer/src/App.tsx (only the layout slots)

Replace your current return with:

  return <LayoutShell
    left={
      <div style={{ display:'flex', flexDirection:'column', height:'100%' }}>
        <div style={{ padding:12, fontWeight:700 }}>KiroClone</div>
        <div style={{ padding:'0 12px 8px' }}>
          <button onClick={openFolder}>Open Folder</button>
        </div>
        <div style={{ padding:'0 8px', color:'var(--muted)' }}>Explorer</div>
        <div style={{ flex:1, overflow:'auto', padding:'8px' }}>
          <FileExplorer tree={tree} onOpen={openRel} />
        </div>
      </div>
    }
    center={
      <div style={{ height:'100%', display:'grid', gridTemplateRows:'40px 1fr 28px' }}>
        <div style={{ padding:'6px 10px', background:'var(--panel)', borderBottom:'1px solid #222', display:'flex', gap:8, alignItems:'center' }}>
          <button onClick={save} disabled={!currentPath}>Save</button>
          <button onClick={saveAs}>Save As</button>
          <div style={{ color:'var(--muted)', marginLeft:12, overflow:'hidden', textOverflow:'ellipsis', whiteSpace:'nowrap' }}>
            {currentPath ?? 'Untitled'}
          </div>
        </div>
        <div><EditorMonaco value={buffer} onChange={setBuffer} language="typescript" /></div>
        <div style={{ padding:'4px 8px', fontSize:12, color: serverOk ? '#3fb950' : '#f85149', background:'var(--panel)', borderTop:'1px solid #222' }}>
          {serverOk === null ? 'Checking server‚Ä¶' : serverOk ? 'Local server OK' : 'Local server not reachable'}
        </div>
      </div>
    }
    right={<ChatSidebar />}
  />;


(Imports at top: add FileExplorer, ChatSidebar, and keep the helpers.)

4) Quick test

Put your keys in .env at repo root:

ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...


Restart server & apps:

pnpm --filter @kiroclone/server dev
pnpm --filter @kiroclone/app-renderer dev
pnpm --filter @kiroclone/app-electron dev


In the right Chat panel:

Provider: Auto

Model: claude-3.5-sonnet

Type: Build a todo app spec ‚Üí Gen Spec

You should get requirements + a Mermaid diagram + a JSON task list.

Flip Provider to OpenAI and Model to gpt-4o, send a short prompt.
If you choose gpt-5 (alias), server maps to gpt-4o until a real gpt-5 exists.

If no keys are set, provider falls back to Mock and echoes a mock reply (so the UI never blocks).

5) What you now have

Server-only AI keys (renderer never sees them).

Provider registry with Anthropic (primary), OpenAI (fallback), stubs for Gemini/Grok.

Chat sidebar with model picker + Gen Spec.

Spec response includes Mermaid + tasks JSON (from LLM).

6) Next (Phase-5 preview)

Specs panel (Markdown/YAML editor + Mermaid live preview + export).

Persist specs to .kiro/specs/*.md|*.yaml + tasks to .kiro/tasks.json.

Approve spec ‚Üí scaffold Jest tests per task.

If anything errors, paste the exact message and file path ‚Äî I‚Äôll patch forward.

You said:
remember chat should be exactly like thsi 
ChatGPT said:

crisp and fast ‚Äî we‚Äôre wiring Phase-4: Chat + Spec generator now, with Claude primary, OpenAI fallback, and a model picker. I‚Äôm also giving you a UI that matches your screenshots: header, messages area, input box, Send / Gen Spec, Provider/Model selectors, and an Autopilot toggle stub.

Follow these steps exactly.

0) Install dependencies

From the repo root:

pnpm add -F @kiroclone/server @anthropic-ai/sdk openai langchain zod sharp
pnpm add -D -F @kiroclone/server @types/sharp

pnpm add -F @kiroclone/app-renderer @reduxjs/toolkit react-redux


Update env template:

# .env.example  (append or ensure these exist)
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
GOOGLE_API_KEY=
XAI_API_KEY=


Put your real keys in .env (never commit it).

1) Server: provider registry + routes
packages/server/src/providers/types.ts
export type ChatMessage = { role: 'system' | 'user' | 'assistant'; content: string };

export type ChatRequest = {
  provider: 'anthropic' | 'openai' | 'gemini' | 'grok' | 'auto' | 'mock';
  model: string;         // e.g., 'claude-3.5-sonnet', 'gpt-4o', 'gpt-5'
  messages: ChatMessage[];
  maxTokens?: number;
};

export type ChatResponse = {
  provider: string;
  model: string;
  content: string;
  usage?: Record<string, unknown>;
  warn?: string;
};

export interface ChatProvider {
  id: string;
  supports(model: string): boolean;
  available(): boolean;
  chat(req: ChatRequest): Promise<ChatResponse>;
}

packages/server/src/providers/anthropic.ts
import Anthropic from '@anthropic-ai/sdk';
import { ChatProvider, ChatRequest, ChatResponse } from './types';

const MAP: Record<string, string> = {
  'claude-sonnet-4': 'claude-3-5-sonnet-20241022', // future alias
  'claude-3.5-sonnet': 'claude-3-5-sonnet-20241022',
  'claude-3.5-sonnet-latest': 'claude-3-5-sonnet-20241022'
};

export class AnthropicProvider implements ChatProvider {
  id = 'anthropic';
  private client?: Anthropic;

  constructor() {
    const key = process.env.ANTHROPIC_API_KEY;
    if (key) this.client = new Anthropic({ apiKey: key });
  }

  available() { return !!this.client; }

  supports(model: string) {
    const m = MAP[model] ?? model;
    return /^claude/i.test(m);
  }

  async chat(req: ChatRequest): Promise<ChatResponse> {
    if (!this.client) {
      return { provider: this.id, model: req.model, content: '(no ANTHROPIC_API_KEY set)', warn: 'provider_unavailable' };
    }
    const model = MAP[req.model] ?? req.model;
    const sys = req.messages.find(m => m.role === 'system')?.content;
    const userTurns = req.messages.filter(m => m.role !== 'system');

    const msg = await this.client!.messages.create({
      model,
      max_tokens: req.maxTokens ?? 2048,
      system: sys,
      messages: userTurns.map(m => ({ role: m.role === 'assistant' ? 'assistant' : 'user', content: m.content }))
    });

    const content = msg.content?.map(c => ('text' in c ? c.text : '')).join('\n') ?? '';
    return { provider: this.id, model, content, usage: { input: msg.usage?.input_tokens, output: msg.usage?.output_tokens } };
  }
}

packages/server/src/providers/openai.ts
import OpenAI from 'openai';
import { ChatProvider, ChatRequest, ChatResponse } from './types';

const MAP: Record<string, string> = {
  'gpt-5': 'gpt-4o', // alias until a real gpt-5 exists
  'gpt-4o': 'gpt-4o',
  'gpt-4o-mini': 'gpt-4o-mini'
};

export class OpenAIProvider implements ChatProvider {
  id = 'openai';
  private client?: OpenAI;

  constructor() {
    const key = process.env.OPENAI_API_KEY;
    if (key) this.client = new OpenAI({ apiKey: key });
  }

  available() { return !!this.client; }

  supports(model: string) {
    const m = MAP[model] ?? model;
    return /^gpt/i.test(m);
  }

  async chat(req: ChatRequest): Promise<ChatResponse> {
    if (!this.client) {
      return { provider: this.id, model: req.model, content: '(no OPENAI_API_KEY set)', warn: 'provider_unavailable' };
    }
    const model = MAP[req.model] ?? req.model;
    const resp = await this.client.chat.completions.create({
      model,
      max_tokens: req.maxTokens ?? 2048,
      messages: req.messages.map(m => ({ role: m.role, content: m.content }))
    });
    const choice = resp.choices?.[0]?.message?.content ?? '';
    return { provider: this.id, model, content: choice, usage: resp.usage as any };
  }
}

packages/server/src/providers/stubs.ts
import { ChatProvider, ChatRequest, ChatResponse } from './types';

export class MockProvider implements ChatProvider {
  id = 'mock';
  available() { return true; }
  supports(_m: string) { return true; }
  async chat(req: ChatRequest): Promise<ChatResponse> {
    const last = req.messages.filter(m => m.role !== 'system').at(-1)?.content ?? '';
    return { provider: 'mock', model: req.model, content: `[MOCK] ${last}` };
  }
}

// Future SDK hookups ‚Äî currently behave like Mock to keep UI working
export class GeminiProvider extends MockProvider { id = 'gemini'; }
export class GrokProvider extends MockProvider { id = 'grok'; }

packages/server/src/providers/index.ts
import { ChatProvider, ChatRequest } from './types';
import { AnthropicProvider } from './anthropic';
import { OpenAIProvider } from './openai';
import { GeminiProvider, GrokProvider, MockProvider } from './stubs';

const providers: ChatProvider[] = [
  new AnthropicProvider(),
  new OpenAIProvider(),
  new GeminiProvider(),
  new GrokProvider(),
  new MockProvider()
];

export function resolveProvider(req: ChatRequest): ChatProvider {
  if (req.provider && req.provider !== 'auto') {
    const p = providers.find(p => p.id === req.provider);
    return p ?? new MockProvider();
  }
  const preferred = ['anthropic','openai','gemini','grok'];
  for (const id of preferred) {
    const p = providers.find(p => p.id === id);
    if (p && p.available() && p.supports(req.model)) return p;
  }
  return providers.find(p => p.available()) ?? new MockProvider();
}

packages/server/src/routes/chat.ts
import { Router } from 'express';
import { z } from 'zod';
import { resolveProvider } from '../providers';
import type { ChatRequest } from '../providers/types';

const ChatSchema = z.object({
  provider: z.enum(['anthropic','openai','gemini','grok','auto','mock']).default('auto'),
  model: z.string().default('claude-3.5-sonnet'),
  messages: z.array(z.object({
    role: z.enum(['system','user','assistant']),
    content: z.string()
  })),
  maxTokens: z.number().int().positive().optional()
});

export const chatRouter = Router();

chatRouter.post('/', async (req, res) => {
  const parsed = ChatSchema.safeParse(req.body);
  if (!parsed.success) return res.status(400).json({ error: parsed.error.flatten() });

  const reqObj = parsed.data as ChatRequest;
  const provider = resolveProvider(reqObj);

  try {
    const out = await provider.chat(reqObj);
    res.json(out);
  } catch (e: any) {
    res.status(500).json({ error: e?.message ?? 'chat_failed', provider: provider.id });
  }
});

packages/server/src/routes/spec.ts
import { Router } from 'express';
import { z } from 'zod';
import { resolveProvider } from '../providers';
import type { ChatRequest } from '../providers/types';

export const specRouter = Router();

const Body = z.object({
  prompt: z.string(),
  provider: z.enum(['anthropic','openai','auto','mock']).default('auto'),
  model: z.string().default('claude-3.5-sonnet'),
});

const SYS = `You are a senior architect.
Given an idea, output:
1) REQUIREMENTS (bullets)
2) SYSTEM DESIGN with a Mermaid diagram (fenced as \`\`\`mermaid ... \`\`\`)
3) TASKS as JSON: [{id,title,description,dependsOn[]}]. Keep it practical.`;

specRouter.post('/', async (req, res) => {
  const parsed = Body.safeParse(req.body);
  if (!parsed.success) return res.status(400).json({ error: parsed.error.flatten() });

  const { prompt, provider: pid, model } = parsed.data;
  const cr: ChatRequest = {
    provider: pid,
    model,
    messages: [
      { role: 'system', content: SYS },
      { role: 'user', content: prompt }
    ]
  };
  try {
    const out = await resolveProvider(cr).chat(cr);
    res.json({ ...out, kind: 'spec' });
  } catch (e: any) {
    res.status(500).json({ error: e?.message ?? 'spec_failed' });
  }
});

packages/server/src/app.ts (replace to mount these routes)
import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import morgan from 'morgan';
import dotenv from 'dotenv';
import { chatRouter } from './routes/chat';
import { specRouter } from './routes/spec';

export function createApp() {
  dotenv.config();
  const app = express();
  app.use(helmet());
  app.use(cors({ origin: ['http://localhost:5173','http://localhost:5174'] }));
  app.use(express.json({ limit: '15mb' }));
  app.use(morgan('tiny'));

  app.get('/health', (_req, res) => res.json({ ok: true, name: 'KiroClone Server', phase: 4 }));

  app.use('/chat', chatRouter);
  app.use('/generate/spec', specRouter);

  return app;
}


(Keep src/index.ts from Phase-3.)

2) Renderer: store + Chat sidebar (like your screenshot)
packages/app-renderer/src/store/index.ts
import { configureStore } from '@reduxjs/toolkit';
import settings from './settingsSlice';
import chat from './chatSlice';

export const store = configureStore({ reducer: { settings, chat } });
export type RootState = ReturnType<typeof store.getState>;
export type AppDispatch = typeof store.dispatch;

packages/app-renderer/src/store/settingsSlice.ts
import { createSlice, PayloadAction } from '@reduxjs/toolkit';

type Provider = 'anthropic' | 'openai' | 'auto' | 'mock';

type SettingsState = {
  provider: Provider;
  model: string;
  autopilot: boolean; // UI toggle
};

const initialState: SettingsState = {
  provider: 'auto',
  model: 'claude-3.5-sonnet',
  autopilot: false
};

const slice = createSlice({
  name: 'settings',
  initialState,
  reducers: {
    setProvider(state, action: PayloadAction<Provider>) { state.provider = action.payload; },
    setModel(state, action: PayloadAction<string>) { state.model = action.payload; },
    toggleAutopilot(state) { state.autopilot = !state.autopilot; }
  }
});

export const { setProvider, setModel, toggleAutopilot } = slice.actions;
export default slice.reducer;

packages/app-renderer/src/store/chatSlice.ts
import { createSlice, PayloadAction } from '@reduxjs/toolkit';

export type Msg = { role: 'user' | 'assistant'; content: string };

type ChatState = { messages: Msg[]; busy: boolean; error?: string };
const initial: ChatState = { messages: [], busy: false };

const slice = createSlice({
  name: 'chat',
  initialState: initial,
  reducers: {
    push(state, action: PayloadAction<Msg>) { state.messages.push(action.payload); },
    setBusy(state, action: PayloadAction<boolean>) { state.busy = action.payload; },
    setError(state, action: PayloadAction<string | undefined>) { state.error = action.payload; },
    clear(state) { state.messages = []; state.error = undefined; }
  }
});

export const { push, setBusy, setError, clear } = slice.actions;
export default slice.reducer;

packages/app-renderer/src/main.tsx (wrap with Provider)
import React from 'react';
import { createRoot } from 'react-dom/client';
import App from './App';
import './styles.css';
import { Provider } from 'react-redux';
import { store } from './store';

declare global { interface Window { kirobridge?: any; } }

createRoot(document.getElementById('root')!).render(
  <Provider store={store}>
    <App />
  </Provider>
);

packages/app-renderer/src/ui/ChatSidebar.tsx
import React, { useState } from 'react';
import { useDispatch, useSelector } from 'react-redux';
import type { RootState } from '../store';
import { push, setBusy, setError, clear } from '../store/chatSlice';
import { setProvider, setModel, toggleAutopilot } from '../store/settingsSlice';

const modelsByProvider: Record<string, string[]> = {
  auto: ['claude-3.5-sonnet','claude-sonnet-4','gpt-4o','gpt-4o-mini','gpt-5'],
  anthropic: ['claude-3.5-sonnet','claude-sonnet-4'],
  openai: ['gpt-4o','gpt-4o-mini','gpt-5'],
  mock: ['mock-echo']
};

export const ChatSidebar: React.FC = () => {
  const dispatch = useDispatch();
  const { messages, busy, error } = useSelector((s: RootState) => s.chat);
  const { provider, model, autopilot } = useSelector((s: RootState) => s.settings);
  const [input, setInput] = useState('');

  const send = async (path: '/chat' | '/generate/spec') => {
    if (!input.trim() || busy) return;
    if (path === '/chat') dispatch(push({ role: 'user', content: input }));
    dispatch(setBusy(true)); dispatch(setError(undefined));
    try {
      const res = await fetch(`http://127.0.0.1:4455${path}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(path === '/chat'
          ? { provider, model, messages: [{ role:'user', content: input }] }
          : { provider, model, prompt: input })
      });
      const data = await res.json();
      if (!res.ok) throw new Error(data?.error || 'request_failed');
      dispatch(push({ role: 'assistant', content: data.content || '' }));
    } catch (e: any) {
      dispatch(setError(e?.message ?? 'request_failed'));
    } finally {
      dispatch(setBusy(false)); setInput('');
    }
  };

  const modelOptions = modelsByProvider[provider] ?? ['claude-3.5-sonnet'];

  return (
    <div style={{ display:'flex', flexDirection:'column', height:'100%' }}>
      {/* Header like your screenshots */}
      <div style={{ display:'flex', alignItems:'center', justifyContent:'space-between', marginBottom:8 }}>
        <div style={{ fontWeight:700 }}>New Session</div>
        <div style={{ display:'flex', gap:8 }}>
          <button onClick={() => dispatch(clear())} disabled={busy}>Clear</button>
          <label style={{ display:'flex', alignItems:'center', gap:6, fontSize:12 }}>
            Auto
            <input type="checkbox" checked={autopilot} onChange={() => dispatch(toggleAutopilot())} />
            Autopilot
          </label>
        </div>
      </div>

      {/* Provider/Model pickers */}
      <div style={{ display:'grid', gridTemplateColumns:'1fr 1fr', gap:6 }}>
        <select value={provider} onChange={e => dispatch(setProvider(e.target.value as any))}>
          <option value="auto">Auto</option>
          <option value="anthropic">Claude</option>
          <option value="openai">OpenAI</option>
          <option value="mock">Mock</option>
        </select>
        <select value={model} onChange={e => dispatch(setModel(e.target.value))}>
          {modelOptions.map(m => <option key={m} value={m}>{m}</option>)}
        </select>
      </div>

      {/* Messages */}
      <div style={{ flex:1, overflow:'auto', marginTop:8, background:'#0b0f14', border:'1px solid #222', padding:8 }}>
        {messages.length === 0 && <div style={{ color:'var(--muted)' }}>Type and Send, or use Gen Spec.</div>}
        {messages.map((m, i) => (
          <div key={i} style={{ margin:'8px 0' }}>
            <div style={{ fontSize:12, color:'#aaa' }}>{m.role}</div>
            <pre style={{ whiteSpace:'pre-wrap', margin:0 }}>{m.content}</pre>
          </div>
        ))}
      </div>
      {error && <div style={{ color:'#f85149', fontSize:12, marginTop:6 }}>{error}</div>}

      {/* Composer */}
      <div style={{ display:'flex', gap:6, marginTop:8 }}>
        <input
          value={input}
          onChange={e => setInput(e.target.value)}
          onKeyDown={e => (e.key === 'Enter' && !e.shiftKey ? (e.preventDefault(), send('/chat')) : null)}
          placeholder="Ask anything‚Ä¶"
          style={{ flex:1, padding:'8px 10px', background:'#0b0f14', border:'1px solid #222', color:'var(--text)' }}
        />
        <button onClick={() => send('/chat')} disabled={busy}>Send</button>
        <button onClick={() => send('/generate/spec')} disabled={busy}>Gen Spec</button>
      </div>
    </div>
  );
};

Use the sidebar in your app (right column)

In packages/app-renderer/src/App.tsx, ensure you import and set:

import { ChatSidebar } from './ui/ChatSidebar';
...
return <LayoutShell left={left} center={center} right={<ChatSidebar />} />;


(Your Phase-3 LayoutShell already supports right content.)

3) Run it
pnpm install

# 3 windows
pnpm --filter @kiroclone/server dev
pnpm --filter @kiroclone/app-renderer dev
pnpm --filter @kiroclone/app-electron dev


Test:

Right sidebar shows New Session, Auto ‚òê Autopilot, pick Claude / claude-3.5-sonnet.

Type: Build a todo app spec ‚Üí click Gen Spec. You should see requirements + a Mermaid diagram + tasks JSON.

Switch provider to OpenAI, pick gpt-4o (or gpt-5 alias ‚Üí maps to 4o until real 5 exists), send a short prompt.

No keys? The UI will still respond via Mock provider ([MOCK] ...) so nothing crashes.